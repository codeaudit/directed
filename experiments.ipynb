{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "This notebook presents the experiments described in the paper:\n",
    "\n",
    "**The Forward-Backward Embedding of Directed Graphs**\n",
    "\n",
    "It compares various embeddings for a clustering task on both the directed graphs and the bipartite graphs of the [Konect](http://konect.uni-koblenz.de/) collection. The datasets are downloaded automatically from this Web site.\n",
    "\n",
    "The notebook was tested with Anaconda3. \n",
    "\n",
    "Make sure you have numpy, pandas, scipy and scikit-learn packages installed before running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import tarfile\n",
    "import glob\n",
    "import shutil\n",
    "import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.cluster.bicluster import SpectralCoclustering\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from time import time\n",
    "\n",
    "from metrics import cocitation_modularity\n",
    "from forwardbackward_embedding import ForwardBackwardEmbedding\n",
    "from spectral_embedding import SpectralEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_graph_df(dataset, url=\"http://konect.uni-koblenz.de/downloads/tsv/\", compression=\"bz2\"):\n",
    "    \"\"\"\n",
    "    Fetches a tsv file from the konect website and returns the edgelist in a pandas dataframe.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: str\n",
    "        the name of the file to download, without extensions\n",
    "    \"\"\"\n",
    "    dataset_filename = dataset + \".tar.\" + compression\n",
    "    download = urllib.request.urlretrieve(url + dataset_filename, dataset_filename)\n",
    "    tf = tarfile.open(dataset_filename, \"r:\" + compression) \n",
    "    tf.extractall()\n",
    "    os.chdir(dataset)\n",
    "    for filename in glob.glob('out.*'):\n",
    "        f = open(filename)\n",
    "        line = f.readline()\n",
    "        graph_type = line.split(' ')[2][:-1]\n",
    "        graph_df = pd.read_table(f,sep = '\\s+',names = ['source','target','weight','time'],comment='%')    \n",
    "        f.close()\n",
    "    tf.close()\n",
    "    os.chdir('..')  \n",
    "    os.remove(dataset_filename)\n",
    "    shutil.rmtree(dataset)\n",
    "    return graph_df, graph_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, name, directed=True, bipartite=False):        \n",
    "        # get the graph\n",
    "        self.name_ = name\n",
    "        self.directed = directed\n",
    "        self.bipartite = bipartite\n",
    "        \n",
    "        self.df, self.type = import_graph_df(name)\n",
    "        row, col, data = self.df['source'].values, self.df['target'].values, np.ones(len(self.df))      \n",
    "        self.n_edges = len(data)\n",
    "             \n",
    "        if bipartite:\n",
    "            self.raw_adj = sparse.csr_matrix((data, (row, col)))[1:,1:]\n",
    "            self.sym_adj = sparse.bmat([[None, self.raw_adj], [self.raw_adj.T, None]], format='csr')\n",
    "            self.n_nodes = self.raw_adj.shape\n",
    "        else:\n",
    "            self.n_nodes = max(max(row), max(col))\n",
    "            self.raw_adj = sparse.csr_matrix((data, (row, col)), shape=(self.n_nodes+1, self.n_nodes+1))[1:,1:]\n",
    "            self.sym_adj = self.raw_adj.maximum(self.raw_adj.T)            \n",
    "        \n",
    "        self.n_clusters = None\n",
    " \n",
    "    def display(self):\n",
    "        print(self.name_+\": {} nodes, {:d} edges, directed: {}.\".format(self.n_nodes, self.n_edges, self.directed))\n",
    "        \n",
    "    def cocitation_modularity(self, partition):\n",
    "        return cocitation_modularity(partition, self.raw_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bipartite graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bipartite_collection = pd.read_csv('konect_bipartite.csv', sep=';')\n",
    "bipartite_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bipartite_datasets(selected_codes = 'all'):\n",
    "    bipartite_datasets = []\n",
    "    if selected_codes == 'all':\n",
    "        list_name = bipartite_collection['Filename'].values\n",
    "    else:\n",
    "        list_name = bipartite_collection['Filename'][bipartite_collection['Code'].isin(selected_codes)]\n",
    "    for i, filename in enumerate(list_name):\n",
    "        print(filename)\n",
    "        bipartite_datasets.append(Dataset(filename, False, True))\n",
    "    return sorted(bipartite_datasets, key=lambda x: x.n_edges)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directed graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "directed_collection = pd.read_csv('konect_directed.csv', sep=';')\n",
    "directed_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_directed_datasets(selected_codes = 'all'):\n",
    "    directed_datasets = []\n",
    "    if selected_codes == 'all':\n",
    "        list_name = directed_collection['Filename'].values\n",
    "    else:\n",
    "        list_name = directed_collection['Filename'][directed_collection['Code'].isin(selected_codes)]\n",
    "    for i, filename in enumerate(list_name):\n",
    "        print(filename)\n",
    "        directed_datasets.append(Dataset(filename, True, False))\n",
    "    return sorted(directed_datasets, key=lambda x: x.n_edges)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Timeout():\n",
    "    \"\"\"Timeout class using ALARM signal.\"\"\"\n",
    "    class Timeout(Exception):\n",
    "        pass\n",
    " \n",
    "    def __init__(self, sec):\n",
    "        self.sec = sec\n",
    " \n",
    "    def __enter__(self):\n",
    "        signal.signal(signal.SIGALRM, self.raise_timeout)\n",
    "        signal.alarm(self.sec)\n",
    " \n",
    "    def __exit__(self, *args):\n",
    "        signal.alarm(0)    # disable alarm\n",
    " \n",
    "    def raise_timeout(self, *args):\n",
    "        raise Timeout.Timeout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def benchmark(datasets, algo, max_time=1000, n_clusters = 10, n_runs=1, output=None):\n",
    "    \"\"\"\n",
    "    Evaluates an algorithm against several datasets by computing the cocitation modularity of the resulting clustering.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets: list of dataset objects\n",
    "    algo: function\n",
    "        algorithm to evaluate, must take a dataset as input and return an array of length dataset.n_nodes\n",
    "    max_time: int or float\n",
    "        maximum time in seconds allowed to algo to return an output\n",
    "    n_clusters: int\n",
    "        number of clusters to compute\n",
    "    n_runs: int\n",
    "        number of runs to perform for the same tuple(dataset, algo), results are then averaged\n",
    "    output: str\n",
    "        name of the output file to save results\n",
    "    \"\"\"\n",
    "    \n",
    "    if output:\n",
    "        output_file = open(output, 'w')\n",
    "        output_file.write('max_time = {}, n_clusters = {}, n_runs = {}\\n'.format(max_time, n_clusters, n_runs))\n",
    "    for dataset in datasets:\n",
    "        if output:\n",
    "            output_file.write(dataset.name_ + '\\n')\n",
    "        dataset.display()\n",
    "        dataset.n_clusters = n_clusters\n",
    "        avg_time, avg_mod = np.zeros(n_runs), np.zeros(n_runs)\n",
    "        for i in range(n_runs):\n",
    "            start_time = time()\n",
    "            has_finished = False\n",
    "            try:\n",
    "                with Timeout(max_time):\n",
    "                    y_pred = algo(dataset)\n",
    "                    has_finished = True\n",
    "            except Timeout.Timeout:\n",
    "                result = 'Timeout'\n",
    "                break\n",
    "            except MemoryError:\n",
    "                result = 'Memory Error'\n",
    "                break\n",
    "            except ValueError:\n",
    "                result = 'Value Error'\n",
    "                break\n",
    "            except:\n",
    "                result = 'Convergence Error'\n",
    "                break\n",
    "                \n",
    "            avg_time[i] = time() - start_time\n",
    "            avg_mod[i] = dataset.cocitation_modularity(y_pred)\n",
    "\n",
    "        if has_finished:\n",
    "            result = 'Average runnig time {:.2f}s. Cocitation modularity: avg = {:.2f}'\\\n",
    "            .format(np.mean(avg_time), np.mean(avg_mod))\n",
    "        if output:\n",
    "            output_file.write(result+'\\n')\n",
    "            output_file.write('\\n')\n",
    "        print(result)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_Id(dataset):\n",
    "    \"\"\"\n",
    "    K-Means on the raw data, i.e, no embedding.\n",
    "    \"\"\"\n",
    "    noembed = MiniBatchKMeans(n_clusters=dataset.n_clusters, batch_size=20000, n_init=10)\n",
    "    return noembed.fit_predict(dataset.raw_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_Dh(dataset):\n",
    "    \"\"\"\n",
    "    Dhillon's spectral co-clustering.\n",
    "    \"\"\"\n",
    "    cocluster = SpectralCoclustering(n_clusters=dataset.n_clusters, svd_method='randomized')\n",
    "    cocluster.fit(dataset.raw_adj)\n",
    "    return cocluster.row_labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_LE(dataset):\n",
    "    \"\"\"\n",
    "    Spectral clustering with K-Means and Laplacian Eigenmaps\n",
    "    \"\"\"\n",
    "    kmeans = MiniBatchKMeans(n_clusters=dataset.n_clusters, batch_size=20000, n_init=10)\n",
    "    lapeigenmaps = SpectralEmbedding(dataset.n_clusters)\n",
    "    lapeigenmaps.fit(dataset.sym_adj)\n",
    "    if dataset.bipartite:\n",
    "        return kmeans.fit_predict(lapeigenmaps.embedding_)[:dataset.n_nodes[0]]\n",
    "    else:\n",
    "        return kmeans.fit_predict(lapeigenmaps.embedding_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_FB(dataset):\n",
    "    \"\"\"\n",
    "    Spectral clustering with K-Means and ForwardBackward embedding\n",
    "    \"\"\"\n",
    "    kmeans = MiniBatchKMeans(n_clusters=dataset.n_clusters, batch_size=20000, n_init=10)\n",
    "    forwardbackward = ForwardBackwardEmbedding(dataset.n_clusters)\n",
    "    forwardbackward.fit(dataset.raw_adj)\n",
    "    return kmeans.fit_predict(normalize(forwardbackward.embedding_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directed graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all datasets:\n",
    "# directed_datasets = get_directed_datasets()\n",
    "\n",
    "directed_datasets = get_directed_datasets(['MS','Mg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(directed_datasets, cluster_Id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(directed_datasets, cluster_LE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(directed_datasets, cluster_FB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bipartite graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all datasets:\n",
    "# bipartite_datasets = get_bipartite_datasets()\n",
    "\n",
    "bipartite_datasets = get_bipartite_datasets(['AC','YG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(bipartite_datasets, cluster_Id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(bipartite_datasets, cluster_Dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(bipartite_datasets, cluster_LE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "benchmark(bipartite_datasets, cluster_FB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiments can also be run on the node2vec embedding. \n",
    "\n",
    "The node2vec package can be downloaded from [here](https://github.com/eliorc/node2vec), or using the command: **pip install node2vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# from node2vec import Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_N2V(dataset):\n",
    "    \"\"\"\n",
    "    Clustering with K-Means and Node2Vec embedding\n",
    "    \"\"\"\n",
    "    kmeans = MiniBatchKMeans(n_clusters=dataset.n_clusters, batch_size=20000, n_init=10)\n",
    "    graph = nx.from_scipy_sparse_matrix(dataset.sym_adj)\n",
    "    node2vec = Node2Vec(graph, dimensions=dataset.n_clusters, walk_length=5, num_walks=5, workers=4) \n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "    \n",
    "    if dataset.bipartite:\n",
    "        n_nodes = dataset.n_nodes[0]\n",
    "    else:\n",
    "        n_nodes = dataset.n_nodes\n",
    "    n2v_embedding = np.zeros((n_nodes, dataset.n_clusters))\n",
    "    for i in range(n_nodes):\n",
    "        n2v_embedding[i] = model.wv[str(i)]\n",
    "    return kmeans.fit_predict(n2v_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#benchmark(directed_datasets, cluster_N2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#benchmark(bipartite_datasets, cluster_N2V)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
